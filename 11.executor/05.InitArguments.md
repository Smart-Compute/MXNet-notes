## InitArguments
* 此时，已经完成ctx/shape/dtype/stype的infer；
* 为节点outputs开辟NDArray数组，初始化inputs/params/auxs及grads的NDArray；
* 模型参数(dense/sparse row)在多个executor之间共享，如 DataParallelExecutorGroup；
```c++
inline void EmplaceBackZeros(const NDArrayStorageType stype, const mxnet::TShape &shape,
                             const Context &ctx, const int dtype,
                             std::vector<NDArray> *vec) {
  if (stype == kDefaultStorage) { // dense初始化为0
    vec->emplace_back(shape, ctx, false, dtype);
    vec->back() = 0;
  } else { // sparse row/csr无数据即为空
    vec->emplace_back(stype, shape, ctx, true, dtype);
  }
}
inline NDArray ReshapeOrCreate(const std::string& name,
                               const mxnet::TShape& dest_arg_shape,
                               const int dest_arg_dtype,
                               const NDArrayStorageType dest_arg_stype,
                               const Context& ctx,
                               std::unordered_map<std::string, NDArray>* shared_buffer,
                               bool enable_row_sparse_sharing) {
  bool stype_shareable = dest_arg_stype == kDefaultStorage; // dense NDArray可共享
  if (enable_row_sparse_sharing) { // 允许sparse row NDArray可共享
    stype_shareable = stype_shareable || dest_arg_stype == kRowSparseStorage;
  }
  auto it = shared_buffer->find(name);
  if (it != shared_buffer->end()) { // shared_buffer找到，进一步看size/dtype/stype
    bool size_shareable = it->second.shape().Size() >= dest_arg_shape.Size();
    if (size_shareable && stype_shareable) {  // memory can be reused
      CHECK_EQ(it->second.dtype(), dest_arg_dtype)
          << "Requested arg array's dtype does not match that of the reusable ndarray";
      CHECK_EQ(it->second.storage_type(), dest_arg_stype)
          << "Requested arg array's stype does not match that of the reusable ndarray";
      return it->second.Reshape(dest_arg_shape); // 大小足够，dtype/stype一致，可以共享
    } else if (stype_shareable) { // 大小不够不能共享
      LOG(WARNING) << "Bucketing: data " << name << " has a shape " << dest_arg_shape
                   << ", which is larger than already allocated shape " << it->second.shape()
                   << ". Need to re-allocate. Consider putting default bucket key to be "
                   << "the bucket taking the largest input for better memory sharing.";
      it->second = InitZeros(dest_arg_stype, dest_arg_shape, ctx, dest_arg_dtype);
      return it->second;
    } else { // csr NDArray不能共享
      return InitZeros(dest_arg_stype, dest_arg_shape, ctx, dest_arg_dtype);
    }
  } else { // shared_buffer找不着
    auto ret = InitZeros(dest_arg_stype, dest_arg_shape, ctx, dest_arg_dtype);
    if (stype_shareable) { // 可共享，添加到shared_buffer
      shared_buffer->emplace(name, ret);
    }
    return ret;
  }
}

void GraphExecutor::InitArguments(const nnvm::IndexedGraph& idx,
                                  const mxnet::ShapeVector& inferred_shapes,
                                  const nnvm::DTypeVector& inferred_dtypes,
                                  const StorageTypeVector& inferred_stypes,
                                  const std::vector<Context>& in_arg_ctxes,
                                  const std::vector<Context>& arg_grad_ctxes,
                                  const std::vector<Context>& aux_state_ctxes,
                                  const std::vector<OpReqType>& grad_req_types,
                                  const std::unordered_set<std::string>& shared_arg_names,
                                  const Executor* shared_exec, // 共享参数的executor
                                  std::unordered_map<std::string, NDArray>* shared_buffer,
                                  std::vector<NDArray>* in_arg_vec,   // 输入参数
                                  std::vector<NDArray>* arg_grad_vec, // 输入参数的梯度
                                  std::vector<NDArray>* aux_state_vec) { // mutable参数
  data_entry_.resize(idx.num_node_entries()); // 开辟NDArray数组空间，用来存储每个节点的outputs
  size_t arg_top = 0, aux_top = 0; // arg和aux下标
  const auto& mutable_nodes = idx.mutable_input_nodes();
  for (size_t i = 0; i < num_forward_inputs_; ++i) { // 遍历forward图的inputs和aux节点
    const uint32_t nid = idx.input_nodes().at(i);
    const uint32_t eid = idx.entry_id(nid, 0); // 节点对应的NDArray下标
    const mxnet::TShape& inferred_shape = inferred_shapes[eid]; // 获取shape
    const int inferred_dtype = inferred_dtypes[eid]; // 获取dtype和stype
    const NDArrayStorageType inferred_stype = (NDArrayStorageType) inferred_stypes[eid];
    const std::string& arg_name = idx[nid].source->attrs.name; // 节点名
    if (mutable_nodes.count(nid)) { // aux节点（如：batch norm的均值和方差）
      if (nullptr != shared_exec) { // 和shared_exec共享，类型大小必须匹配且为dense格式
        const NDArray& aux_nd = shared_exec->aux_state_map().at(arg_name); // 获取共享变量
        CHECK(inferred_stype == kDefaultStorage && aux_nd.storage_type() == kDefaultStorage)
          << "Non-default storage type detected when creating auxilliary NDArray. The allocated "
          << "memory of shared_exec.aux_array cannot be resued for argument: "
          << arg_name << " for the current executor";
        CHECK_EQ(inferred_shape, aux_nd.shape())
          << "Inferred shape does not match shared_exec.aux_array's shape."
             " Therefore, the allocated memory for shared_exec.aux_array cannot"
             " be resued for creating auxilliary NDArray of the argument: "
          << arg_name << " for the current executor";
        CHECK_EQ(inferred_dtype, aux_nd.dtype())
          << "Inferred dtype does not match shared_exec.aux_array's dtype."
             " Therefore, the allocated memory for shared_exec.aux_array cannot"
             " be resued for creating auxilliary NDArray of the argument: "
          << arg_name << " for the current executor";
        aux_state_vec->emplace_back(aux_nd); // 添加到aux_state_vec
      } else { // 新建NDArray，初始化为0
        EmplaceBackZeros(inferred_stype, inferred_shape, aux_state_ctxes[aux_top],
                         inferred_dtype, aux_state_vec);
      }
      data_entry_[eid] = aux_state_vec->back(); // 设置节点NDArray
      aux_state_map_.emplace(arg_name, aux_state_vec->back()); // 记录变量名到NDArray映射
      ++aux_top;
    } else { // 样本数据 or 模型参数
      if (shared_arg_names.count(arg_name)) { // 模型参数可以在(数据并行的)executor间共享
        if (nullptr != shared_exec) { // 和shared_exec共享
          const NDArray& in_arg_nd = shared_exec->in_arg_map().at(arg_name); // 获取共享变量
          auto arg_nd_stype = in_arg_nd.storage_type(); // 获取stype
          /* 模型参数支持dense和sparse row共享 */
          bool shareable_arg_stype = inferred_stype == kDefaultStorage ||
                                     inferred_stype == kRowSparseStorage;
          CHECK(shareable_arg_stype) << "Inferred storage type "
            << common::stype_string(inferred_stype)
            << " does not support memory sharing with shared_exec.arg_array";
          CHECK_EQ(inferred_stype, arg_nd_stype)
            << "Inferred stype does not match shared_exec.arg_array's stype"
               " Therefore, the allocated memory for shared_exec.arg_array cannot"
               " be resued for creating NDArray of the argument "
            << arg_name << " for the current executor";
          CHECK_EQ(inferred_shape, in_arg_nd.shape())
            << "Inferred shape does not match shared_exec.arg_array's shape"
               " Therefore, the allocated memory for shared_exec.arg_array cannot"
               " be resued for creating NDArray of the argument "
            << arg_name << " for the current executor";
          CHECK_EQ(inferred_dtype, in_arg_nd.dtype())
            << "Inferred dtype does not match shared_exec.arg_array's dtype"
               " Therefore, the allocated memory for shared_exec.arg_array cannot"
               " be resued for creating NDArray of the argument "
            << arg_name << " for the current executor";
          in_arg_vec->emplace_back(in_arg_nd);
        } else { // 新建变量，初始化为0
          EmplaceBackZeros(inferred_stype, inferred_shape, in_arg_ctxes[arg_top],
                           inferred_dtype, in_arg_vec);
        }
        if (kNullOp == grad_req_types[arg_top]) { // 不用计算梯度
          arg_grad_vec->emplace_back(); // 新建空NDArray占位
        } else { // 需要计算梯度
          auto grad_oid = grad_store_.size() + num_forward_outputs_; // 获取当前grad output(backward图输出)
          auto grad_eid = idx.entry_id(idx.outputs()[grad_oid]); // 获取entry id
          auto grad_stype = (NDArrayStorageType) inferred_stypes[grad_eid]; // 获取stype
          if (nullptr != shared_exec && grad_stype == kDefaultStorage &&
              shared_exec->arg_grad_map().at(arg_name).storage_type() == kDefaultStorage) {
            // 只允许dense变量共享
            arg_grad_vec->emplace_back(shared_exec->arg_grad_map().at(arg_name));
          } else { // 不能共享变量
            EmplaceBackZeros(grad_stype, inferred_shape, arg_grad_ctxes[arg_top],
                             inferred_dtype, arg_grad_vec);
          }
          grad_store_.emplace_back(grad_req_types[arg_top], arg_grad_vec->back()); // 保存grad
        }
      } else { // 输入数据，当shared_buffer能找到且大小类型匹配时，会共享参数
        bool enable_row_sparse_sharing = true; // 输入允许sparse row共享
        in_arg_vec->emplace_back(ReshapeOrCreate(arg_name, inferred_shape, inferred_dtype,
                                                 inferred_stype, in_arg_ctxes[arg_top],
                                                 shared_buffer, enable_row_sparse_sharing));
        if (kNullOp == grad_req_types[arg_top]) { // 不用计算梯度
          arg_grad_vec->emplace_back(); // 新建空NDArray占位
        } else { // 某些场景下需要计算输入数据的梯度
          auto grad_oid = grad_store_.size() + num_forward_outputs_; // 获取当前grad output(backward图输出)
          auto grad_eid = idx.entry_id(idx.outputs()[grad_oid]); // 获取entry id
          auto grad_stype = (NDArrayStorageType) inferred_stypes[grad_eid]; // 获取stype
          bool enable_row_sparse_sharing = false; // 梯度只许dense共享
          arg_grad_vec->emplace_back(ReshapeOrCreate("grad of " + arg_name, inferred_shape,
                                                     inferred_dtype, grad_stype,
                                                     arg_grad_ctxes[arg_top], shared_buffer,
                                                     enable_row_sparse_sharing));
          grad_store_.emplace_back(grad_req_types[arg_top], arg_grad_vec->back()); // 保存grad
        }
      }
      in_arg_map_.emplace(arg_name, in_arg_vec->back()); // 记录输入节点名到输入NDArray映射
      if (!arg_grad_vec->back().is_none()) { // 需要计算梯度
        arg_grad_map_.emplace(arg_name, arg_grad_vec->back()); // 记录输入节点名到梯度NDArray映射
      }
      data_entry_[eid] = in_arg_vec->back(); // 设置节点NDArray
      ++arg_top;
    }
  }
}
```
