## AssignContext
* 根据配置为计算图的每个节点分配计算设备；
* 参见[PlaceDevice](https://github.com/zhuzilong-cn/MXNet-notes/blob/master/10.nnvm/09.place_device.md)；
```c++
using ContextVector = std::vector<Context>;
using DeviceVector = std::vector<int>;
using DeviceAssignMap = std::unordered_map<std::string, int>;

inline Graph PlaceDevice(Graph graph,
                         std::string device_group_attr_key, // __ctx_group__
                         DeviceAssignMap device_assign_map, // group2id
                         std::string device_copy_op) { // _CrossDeviceCopy
  graph.attrs["device_group_attr_key"] = std::make_shared<any>(std::move(device_group_attr_key));
  graph.attrs["device_assign_map"] = std::make_shared<any>(std::move(device_assign_map));
  graph.attrs["device_copy_op"] = std::make_shared<any>(std::move(device_copy_op));
  return ApplyPass(std::move(graph), "PlaceDevice");
}

/* 根据forward图输入节点/输出梯度节点的ctx配置、group2ctx和default_ctx为整个计算图的节点分配计算设备 */
inline nnvm::Graph AssignContext(nnvm::Graph g,
                                 const Context& default_ctx, // 默认context
                                 const std::map<std::string, Context>& ctx_map, // group2ctx
                                 const std::vector<Context>& in_arg_ctxes, // 输入节点
                                 const std::vector<Context>& arg_grad_ctxes, // 待计算梯度节点
                                 const std::vector<Context>& aux_state_ctxes, // 辅助节点
                                 const std::vector<OpReqType>& grad_req_types, // 梯度输出方式
                                 size_t num_forward_inputs, // forward图的输入数
                                 size_t num_forward_outputs) { // forward图的输出数
  const auto& idx = g.indexed_graph();
  const auto& mutable_nodes = idx.mutable_input_nodes();
  // default use default context.
  if (ctx_map.size() == 0) { // group2ctx map为空，使用default_ctx
    g.attrs["context"] = std::make_shared<nnvm::any>(
        exec::ContextVector(idx.num_nodes(), default_ctx));
    for (const auto& x : in_arg_ctxes) { // 一致性检查
      CHECK(x == default_ctx)
          << "Input array is in " << x << " while binding with ctx=" << default_ctx
          << ". All arguments must be in global context (" << default_ctx
          << ") unless group2ctx is specified for cross-device graph.";
    }
    for (const auto& x : arg_grad_ctxes) { // 一致性检查
      CHECK(x == default_ctx)
          << "Gradient array is in " << x << " while binding with ctx="
          << default_ctx << ". All gradients must be in global context (" << default_ctx
          << ") unless group2ctx is specified for cross-device graph.";
    }
    return g;
  }
 
  std::map<Context, int> ctx2id;  // ctx到id的映射表
  std::vector<Context> ctx_list;  // id(index)到ctx的映射表
  nnvm::DeviceVector device(idx.num_nodes(), -1);  // 为每个节点分配设备id，默认为-1；
  nnvm::DeviceAssignMap device_map;  // group2ctx => group2id

  /* 遍历group2ctx，为ctx分配id，生成group2id映射表 */
  for (auto &kv : ctx_map) {
    if (ctx2id.count(kv.second) == 0) { // 为新ctx分配id
      ctx2id[kv.second] = static_cast<int>(ctx_list.size());
      ctx_list.push_back(kv.second);
    }
    device_map[kv.first] = ctx2id.at(kv.second); // group => ctx id
  }

  /* 遍历forward图的input节点，为其分配设备id */
  size_t arg_top = 0, aux_top = 0;
  for (size_t i = 0; i < num_forward_inputs; ++i) {
    const uint32_t nid = idx.input_nodes().at(i); // input节点
    Context ctx;
    if (mutable_nodes.count(nid)) {  // 辅助节点
      CHECK_LT(aux_top, aux_state_ctxes.size()); // 下标检查
      ctx = aux_state_ctxes[aux_top]; // 获取ctx
      ++aux_top;
    } else {  // 输入节点
      CHECK_LT(arg_top, in_arg_ctxes.size()); // 下标检查
      ctx = in_arg_ctxes[arg_top]; // 获取ctx
      ++arg_top;
    }
    if (ctx2id.count(ctx) == 0) { // 为新ctx分配id
      ctx2id[ctx] = static_cast<int>(ctx_list.size());
      ctx_list.push_back(ctx);
    }
    device[nid] = ctx2id.at(ctx);  // 为input节点分配设备id
  }

  /* g.outputs不包含req_type为kNullOp的grad */
  CHECK_GE(grad_req_types.size(), g.outputs.size() - num_forward_outputs)
      << "insufficient number of grad_reqs";
  /* 遍历待计算的梯度节点，为其分配设备id */
  size_t arg_grad_offset = 0;
  for (size_t i = num_forward_outputs; i < g.outputs.size(); ++i, ++arg_grad_offset) {
    while (grad_req_types[arg_grad_offset] == kNullOp) ++arg_grad_offset;
    const uint32_t nid = idx.outputs()[i].node_id; // output node
    Context ctx = arg_grad_ctxes[arg_grad_offset]; // 获取output相应的grad ctx
    if (ctx2id.count(ctx) == 0) { // 为新ctx分配id
      ctx2id[ctx] = static_cast<int>(ctx_list.size());
      ctx_list.push_back(ctx);
    }
    int devid = ctx2id.at(ctx); // 获取ctx id
    if (device[nid] != -1) { // 已经分配设备id，校准是否一致
      CHECK_EQ(device[nid], devid) << "device of same output not equal to each other";
    } else { // 分配设备id
      device[nid] = devid;
    }
  }

  /* 完成forward输入节点和输出的梯度节点的设备id设置、group2id映射表生成 */
  g.attrs["device"] = std::make_shared<dmlc::any>(std::move(device));
  /* 遍历图为所有节点分配设备id，当两个相邻节点设备id不一致时，可能会导致数据的跨设备拷贝 */
  g = nnvm::pass::PlaceDevice(g, "__ctx_group__", device_map, "_CrossDeviceCopy");
  /* 图结构可能会被修改，获取各节点的设备id */
  const auto& assigned_devices = g.GetAttr<nnvm::DeviceVector>("device");

  exec::ContextVector vcontext; // device => context
  for (auto context : assigned_devices
