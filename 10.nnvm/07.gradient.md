## MXGradient
* 输入一个计算图，自动实现反向传播计算梯度，输出带forward和backward的完整计算图；
* 步骤1: 聚合下游传入的梯度；
* 步骤2: 利用op的求导函数及链式法则，求解输入的梯度；
* 步骤3: 将梯度反向传递给上游；
* 完成backward计算，需要传入forward图outputs对应的梯度，即grad_ys_out_grad；
```c++
struct GradEntry { // 表示symbol output的梯度
  NodeEntry sum{nullptr, 0, 0}; // 梯度聚合后的结果
  std::vector<NodeEntry> grads; // 记录下游op反向传播的梯度列表
  bool need_attr_hint{true};    // 启用attr_hint_fun的标记
};

/* 输入一个计算图，自动实现反向传播计算梯度，输出带forward和backward的完整计算图 */
Graph Gradient(Graph src) { // 参见 MXGradient()
  using nnvm::FGradient;
  /* Aggregation function applied to aggregate the inputs. */
  using AggFun = std::function<NodeEntry (std::vector<NodeEntry>&& inputs)>;
  /* mirror function to do mirror optimization and save memory. */
  using MirrorFun = std::function<int (const Node& node)>;
  /* hint function to output a node that like src, but its attr is same as like. */
  using AttrHintFun = std::function<NodeEntry (const NodeEntry& src, const NodeEntry &like)>;

  CHECK_NE(src.attrs.count("grad_ys"), 0U)
      << "Gradient require grad_ys to be presented.";
  CHECK_NE(src.attrs.count("grad_ys_out_grad"), 0U)
      << "Gradient require grad_ys_out_grad to be presented.";
  CHECK_NE(src.attrs.count("grad_xs"), 0U)
      << "Gradient require grad_xs to be presented.";

  const std::vector<NodeEntry>& ys = // 输出
      src.GetAttr<std::vector<NodeEntry> >("grad_ys");
  const std::vector<NodeEntry>& ys_out_grad = // 传递给ys的梯度
      src.GetAttr<std::vector<NodeEntry> >("grad_ys_out_grad");
  const std::vector<NodeEntry>& xs = // 输入
      src.GetAttr<std::vector<NodeEntry> >("grad_xs");

  AggFun agg_fun = DefaultAggregateGradient; // 默认elemwise_sum
  if (src.attrs.count("grad_aggregate_fun") != 0) {
    agg_fun = src.GetAttr<AggFun>("grad_aggregate_fun");
  }
  MirrorFun mirror_fun = nullptr;
  if (src.attrs.count("grad_mirror_fun") != 0) {
    mirror_fun = src.GetAttr<MirrorFun>("grad_mirror_fun");
  }
  AttrHintFun attr_hint_fun = nullptr;
  if (src.attrs.count("attr_hint_fun") != 0) {
    attr_hint_fun = src.GetAttr<AttrHintFun>("attr_hint_fun");
  }
  /* list of operators that outputs a single zero array. The first one must be zeros_like. */
  std::vector<const Op*> zero_ops;
  if (src.attrs.count("zero_ops") != 0) {
    zero_ops = src.GetAttr<std::vector<const Op*> >("zero_ops");
  }
  /* name of the copy operation required to handle duplicates on the edge of the graph. */
  const Op* copy_op = (src.attrs.count("copy_op") != 0) ?
      Op::Get(src.GetAttr<std::string>("copy_op")) :
      nullptr;

  std::vector<NodePtr> topo_order; // 深度优先后序遍历得到节点序列
  /* 记录每个节点输出的梯度(vec大小等于该节点的outputs大小) */
  std::unordered_map<Node*, std::vector<GradEntry> > output_grads;

  DFSVisit(ys, [&](const NodePtr& node) {
      if (output_grads.count(node.get()) == 0) {
        output_grads[node.get()].resize(node->num_outputs()); // 梯度映射表
      }
      topo_order.push_back(node); // 记录遍历顺序
    });

  CHECK_EQ(ys.size(), ys_out_grad.size()); // ys与ys_out_grad大小一致
  for (size_t i = 0; i < ys.size(); ++i) {
    /* 初始化计算图output对应的梯度，由外部传入 */
    NodeEntry ograd = ys_out_grad[i]; // 对应ys[i]：节点ys[i].node的第ys[i].index个输出
    output_grads[ys[i].node.get()][ys[i].index].grads = { ograd };
  }

  for (size_t i = 0; i < xs.size(); ++i) { // 检查xs是计算图的节点，否则没法算xs的梯度
    CHECK(output_grads.find(xs[i].node.get()) != output_grads.end())
        << "Cannot differentiate with respect to the " << i+1 << "-th variable "
        << "because it is unreachable from the outputs.";
  }

  std::unordered_map<Node*, NodePtr> mirror_map; // 节点镜像映射表
  if (mirror_fun != nullptr) { // memory reduction strategy
    for (const NodePtr& node_ptr : topo_order) { // 后序遍历序
      if (mirror_fun(*node_ptr)) { // 当前节点是否要创建镜像节点
        NodePtr new_node = Node::Create();
        *new_node = *node_ptr; // 拷贝
        new_node->attrs.name += "_mirror"; // 镜像节点加后缀
        for (auto& e : new_node->inputs) {
          e.node = mirror_map.at(e.node.get()); // inputs已添加到mirror_map
        }
        for (auto& n : new_node->control_deps) {
          n = mirror_map.at(n.get()); // deps已添加到mirror_map(后序遍历)
        }
        mirror_map[node_ptr.get()] = std::move(new_node); // 建立镜像映射
      } else {
        mirror_map[node_ptr.get()] = node_ptr; // 无需创建镜像节点，维持不变
      }
    }
  }

  /* Get the gradient node of the op node. */
  using FGradient = std::function<std::vector<NodeEntry>( // 返回节点输入的梯度
      const NodePtr& nodeptr, // 求梯度的节点
      const std::vector<NodeEntry>& out_grads)>; // 节点输出的梯度
  static auto& grad_fun_map = Op::GetAttr<FGradient>("FGradient");
  /* Shape inference function. */
  using FInferShape = std::function<bool (const NodeAttrs& attrs,
                                          std::vector<TShape> *in_attrs,
                                          std::vector<TShape> *out_attrs)>;
  static auto& finfer_shape = Op::GetAttr<FInferShape>("FInferShape");

  std::vector<NodeEntry> out_agg_grads;
  /* 前序遍历计算图(后序遍历的逆序)，构造反向传播计算图(初始梯度已设置) */
  for (auto rit = topo_order.rbegin(); rit != topo_order.rend(); ++rit) {
    const NodePtr& ptr = *rit;
    if (ptr->is_variable()) continue; // 梯度已经传递到var
    /* 分3个步骤实现op反向传播逻辑 */
    /* 步骤1: 聚合下游传入的梯度； */
    out_agg_grads.clear(); // 清空准备收集op输出的梯度
    auto& out_grad_vec = output_grads.at(ptr.get()); // 当前op输出对应的梯度
    for (uint32_t i = 0; i < out_grad_vec.size(); ++i) { // 聚合反向传播的梯度(多个下游)
      GradEntry& e = out_grad_vec[i]; // 当前op的第i个输出
      /* 该输出可能是多个下游op的输入，根据复合函数链式求导法则，应当聚合反向传播的梯度(相加) */
      e.sum = agg_fun(std::move(e.grads)); // 聚合反向传播的梯度
      if (e.need_attr_hint && attr_hint_fun != nullptr) {
        e.sum = attr_hint_fun(e.sum, NodeEntry{ptr, 0, i}); // 利用ptr赋值attributes
      }
      out_agg_grads.push_back(e.sum); // 收集第i个输出的梯度
    }
    if ((*rit)->inputs.size() != 0) { // 需要继续把梯度反向传播给op的输入
      /* 步骤2: 利用op的求导函数及链式法则，求解输入的梯度；*/
      /* 获取当前op对应的(镜像)节点 */
      NodePtr fwd_node = (mirror_map.size() == 0 ? ptr : mirror_map.at(ptr.get()));
      std::vector<NodeEntry> input_grads; // op输入的梯度，待计算
      if (grad_fun_map.contains(ptr->op())) { // 该op定义了求导函数
        /* 根据op节点及对应输出的梯度反向计算op输入的梯度，实现链式法则 */
        input_grads = grad_fun_map[ptr->op()](fwd_node, out_agg_grads);
        CHECK_EQ((*rit)->inputs.size(), input_grads.size()) // 每个输入都有求解梯度
            << "Gradient function not returning enough gradient";
      } else if (CheckGradAllZero(out_agg_grads, zero_ops)) {
        /* 如果当前op所有输出的梯度都为0，根据复合函数链式法则，op所有输入的梯度也都为0 */
        for (size_t i = 0; i < fwd_node->num_inputs(); ++i) {
          std::ostringstream os; // 自动生成节点名
          if (1 == fwd_node->num_inputs()) {
            os << fwd_node->attrs.name << "_backward";
          } else {
            os << fwd_node->attrs.name << "_in" << i << "_backward";
          }
          auto p = Node::Create();
          p->attrs.op = zero_ops[0]; // 约定此处必须为zeros_like
          p->attrs.name = os.str();  // 设置节点名
          p->inputs.push_back(fwd_node->inputs[i]); // 设置input，以便infer shape
          p->control_deps.emplace_back(fwd_node);   // 执行顺序依赖
          if (p->op()->attr_parser != nullptr) {    // 自动parse属性
            p->op()->attr_parser(&(p->attrs));
          }
          input_grads.emplace_back(p, 0, 0); // 生成op第i个输入的梯度(全为0)
        }
      } else { /* op未定义求导函数，无法生成反向传播计算图 */
        LOG(FATAL) << "Operator " << fwd_node->op()->name << " is non-differentiable "
                   << "because it didn't register FGradient attribute.";
      }
      /* 步骤3: 将梯度反向传递给上游；*/
      /* 检查input_grads必须有设置node */
      for (const auto& nodeEntry : input_grads) CHECK(nodeEntry.node);
      /* 把当前op输入的梯度传递给上游op对应的输出 */
      auto git = input_grads.begin();
      CHECK((*rit)->inputs.size() <= input_grads.size());
      for (auto it = (*rit)->inputs.begin(); it != (*rit)->inputs.end(); ++it, ++git) {
        auto& output_grad_entry = output_grads[it->node.get()][it->index];
        if (finfer_shape.contains(git->node->op())) {
          // 如果反向传播op定义了shape inference，无需hint
          output_grad_entry.need_attr_hint = false;
        }
        // 将梯度反向传播给上游输出，以便上游输出聚合梯度继续反向求导
        output_grad_entry.grads.emplace_back(std::move(*git));
      }
    }
  }

  template<typename ValueType>  
  using NodeEntryMap = std::unordered_map<NodeEntry, ValueType, NodeEntryHash, NodeEntryEqual>;

  Graph ret; // 获取xs的梯度作为整个计算图的输出
  ret.outputs.resize(xs.size());
  NodeEntryMap<std::pair<size_t, size_t> > unique_grads;
  size_t counter = 0; // xs下标
  for (const NodeEntry& e : xs) {
    GradEntry& entry = output_grads[e.node.get()][e.index]; // xs[i]的梯度
    if (entry.sum.node.get() == nullptr) { // 聚合梯度
      entry.sum = agg_fun(std::move(entry.grads));
      if (entry.need_attr_hint && attr_hint_fun != nullptr) {
        entry.sum = attr_hint_fun(entry.sum, e);
      }
    }
    if (copy_op != nullptr) { // 当出xs有元素重复时，提供copy op避免重复聚合梯度，可能更高效
      auto kv = unique_grads.find(entry.sum);
      if (kv == unique_grads.end()) { // 首次录入
        unique_grads.emplace(std::move(entry.sum), std::make_pair(1, counter));
      } else {
        NodePtr copy_node = Node::Create();
        std::ostringstream os; // copy节点的命名规则
        os << entry.sum.node->attrs.name << "_" << kv->second.first << "_copy";
        kv->second.first++; // 重复(copy)计算
        copy_node->attrs.op = copy_op; // copy op
        copy_node->attrs.name = os.str();
        copy_node->inputs.emplace_back(entry.sum); // inputs to copy op
        if (copy_node->attrs.op->attr_parser != nullptr) {
            copy_node->attrs.op->attr_parser(&(copy_node->attrs));
        }
        unique_grads.emplace(NodeEntry{std::move(copy_node), 0, 0}, std::make_pair(1, counter));
      }
    } else {
      ret.outputs[counter] = entry.sum; // 记录xs[i]的梯度输出
    }
    ++counter;
  }
  if (copy_op != nullptr) {
    for (const auto& kv : unique_grads) {
      ret.outputs[kv.second.second] = kv.first; // 设置xs的梯度输出
    }
  }
  return ret; /* 输出带反向传播的计算图(包含forward和backward) */
}

/* 默认实现梯度相加：复合函数求导链式法则 */
NodeEntry DefaultAggregateGradient(std::vector<NodeEntry>&& v) {
  if (v.size() == 1) { // 不用聚合
    return std::move(v[0]);
  } else if (v.size() == 0) { // 无梯度传入默认为0
    NodePtr zero_node = Node::Create();
    zero_node->attrs.op = Op::Get("zeros");
    zero_node->attrs.name = "zero_grad";
    zero_node->attrs.op->attr_parser(&(zero_node->attrs));
    return NodeEntry{zero_node, 0, 0};
  } else { // 梯度相加
    NodePtr sum_node = Node::Create();
    sum_node->attrs.op = Op::Get("elemwise_sum");
    sum_node->inputs = std::move(v);
    sum_node->attrs.name = "grad_sum";
    sum_node->attrs.dict["num_args"] = std::to_string(sum_node->inputs.size());
    sum_node->attrs.op->attr_parser(&(sum_node->attrs));
    return NodeEntry{sum_node, 0, 0};
  }
}
/* 检查所有梯度的op是否都属于zero_ops，即所有梯度均为0 */
bool CheckGradAllZero(const std::vector<NodeEntry>& grads,
                      const std::vector<const Op*>& zero_ops) {
  if (!grads.size() || !zero_ops.size()) return false;
  for (const auto& g : grads) {
    bool found = false;
    for (const auto& op : zero_ops) {
      if (g.node->op() == op) {
        found = true;
        break;
      }
    }
    if (!found) return false;
  }
  return true;
}

NNVM_REGISTER_PASS(MXGradient) // 注册Gradient函数，提供自动生成反向传播计算图的功能
.describe("Return a gradient graph of src.attrs[\"ys\"] wrt src.attrs[\"xs\"]")
.set_body(Gradient)
.set_change_graph(true)
.depend_graph_attr("grad_ys")
.depend_graph_attr("grad_xs")
.depend_graph_attr("grad_ys_out_grad");

inline Graph MXGradient( // 使用接口
    Graph graph,
    std::vector<NodeEntry> ys,
    std::vector<NodeEntry> xs,
    std::vector<NodeEntry> ys_out_grad,
    std::function<NodeEntry(std::vector<NodeEntry>&& inputs)> aggregate_fun = nullptr,
    std::function<int(const Node& node)> mirror_fun = nullptr,
    std::function<NodeEntry(const NodeEntry& src, const NodeEntry &like)>
    attr_hint_fun = nullptr,
    std::vector<const Op*> zero_ops = std::vector<const Op*>(),
    std::string copy_op_str = std::string()) {
  graph.attrs["grad_ys"] = std::make_shared<any>(std::move(ys));
  graph.attrs["grad_xs"] = std::make_shared<any>(std::move(xs));
  graph.attrs["grad_ys_out_grad"] = std::make_shared<any>(std::move(ys_out_grad));
  if (aggregate_fun != nullptr) {
    graph.attrs["grad_aggregate_fun"] = std::make_shared<any>(aggregate_fun);
  }
  if (mirror_fun != nullptr) {
    graph.attrs["grad_mirror_fun"] = std::make_shared<any>(mirror_fun);
  }
  if (attr_hint_fun != nullptr) {
    graph.attrs["attr_hint_fun"] = std::make_shared<any>(attr_hint_fun);
  }
  if (zero_ops.size()) {
    graph.attrs["zero_ops"] = std::make_shared<any>(std::move(zero_ops));
  }
  if (copy_op_str != std::string()) {
    graph.attrs["copy_op"] = std::make_shared<any>(std::move(copy_op_str));
  }
  return ApplyPass(std::move(graph), "MXGradient");
}
```
