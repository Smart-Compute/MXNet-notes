## TBlob
* TBlob对连续存储的tensor进行封装，支持原始指针+shape+type构造、DLTensor构造、mshadow::Tensor构造；
* 支持reshape、升维和降维，返回mshadow::Tensor；
```
 enum TypeFlag {
   kFloat32 = 0,
   kFloat64 = 1,
   kFloat16 = 2,
   kUint8 = 3,
   kInt32 = 4,
   kInt8  = 5,
   kInt64 = 6,
 };

 /*
  * TBlob can be used to hold tensor of any dimension, any device and any data type.
  */
 class TBlob {
   friend class NDArray;
  public:
   void *dptr_;          /*! \brief pointer to the data */
   mxnet::TShape shape_; /*! \brief shape of the tensor */
   int type_flag_;       /*! \brief type flag of the tensor blob */

   TBlob(void *dptr, const mxnet::TShape &shape, int dev_mask, int type_flag, int dev_id = -1)
       : dptr_(dptr), shape_(shape), type_flag_(type_flag) {
     SetDLTensor(dev_mask, dev_id);
   }
   explicit TBlob(const DLTensor &dltensor)
       : dptr_(dltensor.data),
         shape_(mxnet::TShape(dltensor.shape, dltensor.shape + dltensor.ndim)),
         type_flag_(DLDataTypeTransform(dltensor.dtype)),
         dltensor_(dltensor) {
     /* 检查DLTensor是否为紧密(连续)存储 */
     if (dltensor.strides != nullptr) { // check strides
       const int &ndim = dltensor.ndim;
       const int64_t *shape = dltensor.shape;
       const int64_t *strides = dltensor.strides;
       if (ndim >= 1) {
         bool err = false;
         if (strides[ndim - 1] != 1) { // 要求最低维度步长为1，否则不是紧密(连续)存储
           err = true;
         } else {
           for (int i = ndim - 2; i >= 0; --i) {
             if (strides[i] != shape[i + 1] * strides[i + 1]) { // 检查各维度步长合法性
               err = true;
               break;
             }
           }
         }
         if (err) {
           LOG(FATAL) << "Unsupported DLPack because MXNet only support compact tensor now";
         }
       }
     }
   }
   template<typename Device, int dim, typename DType>
   TBlob(const mshadow::Tensor<Device, dim, DType> &src) { *this = src; }

   template<typename Device, int dim, typename DType>
   inline TBlob &operator=(const mshadow::Tensor<Device, dim, DType> &src) {
     dptr_ = src.dptr_;
     shape_ = src.shape_;
     type_flag_ = mshadow::DataType<DType>::kFlag;
     SetDLTensor(Device::kDevMask, -1);
     return *this;
   }

   inline bool CheckContiguous(void) const { return true; }

   inline TBlob reshape(const mxnet::TShape& shape) const{
     CHECK_EQ(this->shape_.Size(), shape.Size()) << "Shape size mismatch "
              << this->shape_.Size() << " v.s. "  << shape.Size();
     TBlob ret(this->dptr_, shape, this->dev_mask(), this->type_flag_, this->dev_id());
     return ret;
   }

   inline int ndim(void) const { return shape_.ndim(); }
   inline index_t size(index_t idx) const { return shape_[idx]; }
   inline size_t Size(void) const { return shape_.Size(); }
   inline int dev_mask() const { return dltensor_.ctx.device_type; }
   inline int dev_id() const { return dltensor_.ctx.device_id; }
   inline const DLTensor& dltensor() const { return dltensor_; }

   template<typename DType>
   inline DType* dptr() const {
     CHECK(mshadow::DataType<DType>::kFlag == type_flag_)
           << "TBlob.get_with_shape: data type do not match specified type."
           << "Expected: " << type_flag_ << " v.s. given " << mshadow::DataType<DType>::kFlag;
     return static_cast<DType*>(dptr_);
   }

   template<typename Device, int dim, typename DType>
   inline mshadow::Tensor<Device, dim, DType> get(
       mshadow::Stream<Device> *stream = NULL) const { // dim必须等于shape_.ndim()，否则会报错；
     CHECK(Device::kDevMask == this->dev_mask())
           << "TBlob.get: device type do not match specified type";
     return mshadow::Tensor<Device, dim, DType>(dptr<DType>(),
         shape_.get<dim>(), // mxnet::TShape转换为mshadow::Shape<dim>
         shape_[shape_.ndim() - 1], stream);
   }

   template<typename Device, int dim, typename DType>
   inline mshadow::Tensor<Device, dim, DType> get_with_shape(
       const mshadow::Shape<dim> &shape, mshadow::Stream<Device> *stream = NULL) const {
     CHECK(Device::kDevMask == this->dev_mask())
         << "TBlob.get: device type do not match specified type";
     CHECK_EQ(this->CheckContiguous(), true) << "TBlob.get_reshape: must be contiguous";
     CHECK_EQ(this->shape_.Size(), static_cast<size_t>(shape.Size()))
              << "TBlob.get_with_shape: new and old shape do not match total elements";
     return mshadow::Tensor<Device, dim, DType>(dptr<DType>(), shape, shape[dim - 1], stream);
   }
   
   template<typename Device, typename DType>
   inline mshadow::Tensor<Device, 1, DType> FlatTo1D(mshadow::Stream<Device> *stream = NULL) const;
   template<typename Device, typename DType>
   inline mshadow::Tensor<Device, 2, DType> FlatTo2D(mshadow::Stream<Device> *stream = NULL) const;
   template<typename Device, typename DType>
   inline mshadow::Tensor<Device, 3, DType> FlatTo3D(
       int axis, mshadow::Stream<Device> *stream = NULL) const;
   template<typename Device, typename DType>
   inline mshadow::Tensor<Device, 3, DType> FlatTo3D(
       int axis_begin, int axis_end, mshadow::Stream<Device> *stream = NULL) const;

   template<typename Device, int dim, typename DType>
   inline mshadow::Tensor<Device, dim, DType> FlatToKD(
       mshadow::Stream<Device> *stream = NULL) const { /* 升维：高维度补1；降维：高维度合并； */
     mshadow::Shape<dim> shape;
     shape[0] = 1;
     for (int i = 0; i < dim - ndim(); ++i) {     // Pad higher dimensions in case dim > ndim()
       shape[i] = 1;
     }
     for (int i = 0; i < ndim() - dim + 1; ++i) { // Collapse higher dimensions in case dim < ndim()
       shape[0] *= shape_[i];
     }
     for (int i = std::max(0, ndim() - dim + 1); i < ndim(); ++i) { // Preserve lower dimensions.
       shape[i - ndim() + dim] = shape_[i];
     }
     return this->get_with_shape<Device, dim, DType>(shape, stream);
   }

  private:
   inline void SetDLTensor(int dev_mask, int dev_id) {
     dltensor_.data = dptr_;
     dltensor_.ctx = DLContext{static_cast<DLDeviceType>(dev_mask), dev_id};
     dltensor_.ndim = shape_.ndim();
     dltensor_.dtype = DTypeTransform(type_flag_);
     dltensor_.shape = shape_.data();
     dltensor_.strides = nullptr;
     dltensor_.byte_offset = 0;
   }

   /* type_flag互转 */
   static DLDataType DTypeTransform(int type_flag);
   static int DLDataTypeTransform(DLDataType dldata_type);

   DLTensor dltensor_; /*! \brief corresponding DLTensor of this TBlob */
 };
```
