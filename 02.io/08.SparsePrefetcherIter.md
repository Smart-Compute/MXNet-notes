## SparsePrefetcherIter
继承PrefetcherIter，封装TBlobBatch的sparse迭代器，实现sparse NDArray构造及数据装载，和父类一样支持prefetch功能；
```
 class SparsePrefetcherIter : public PrefetcherIter {
  public:
   explicit SparsePrefetcherIter(SparseIIterator<TBlobBatch>* base)
       : PrefetcherIter(base), sparse_loader_(base) {}
 
   ~SparsePrefetcherIter() {}
 
   virtual void Init(const std::vector<std::pair<std::string, std::string> >& kwargs) {
     PrefetcherIter::InitParams(kwargs);
     // use the kwarg to init batch loader
     sparse_loader_->Init(kwargs);
     iter.Init([this](DataBatch **dptr) {
         if (!sparse_loader_->Next()) return false;
         const TBlobBatch& batch = sparse_loader_->Value();
         if (*dptr == nullptr) { // allocate databatch
           *dptr = new DataBatch();
           (*dptr)->num_batch_padd = batch.num_batch_padd;
           (*dptr)->data.resize(2); // data + label
           (*dptr)->index.resize(batch.batch_size);
           size_t data_iter = 0;
           for (size_t i = 0; i < (*dptr)->data.size(); ++i) { // 2轮循环
             bool is_data = i == 0;
             auto stype = this->GetStorageType(is_data);
             auto dtype = param_.dtype ? param_.dtype.value() : batch.data[data_iter].type_flag_;
             // dtype设置错误会报类型不匹配错误；param_.ctx不生效？
             if (stype == kDefaultStorage) { // 默认存储：dynamic NDArray
               (*dptr)->data.at(i) = NDArray(batch.data[data_iter].shape_,
                                             Context::CPU(), false, dtype);
             } else { // CSR存储：aux_types = {mshadow::kInt64, mshadow::kInt64};
               (*dptr)->data.at(i) = NDArray(stype, this->GetShape(is_data),
                                             Context::CPU(), false, dtype);
             }
             data_iter += num_aux_data(stype) + 1;
           }
         }
         // copy data over
         size_t data_iter = 0;
         for (size_t i = 0; i < (*dptr)->data.size(); ++i) {
           auto& nd = ((*dptr)->data)[i];
           auto stype = nd.storage_type();
           auto& data_i = ((*dptr)->data)[i]; // 和nd冗余
           if (stype == kDefaultStorage) {
             CopyFromTo(data_i.data(), batch.data[data_iter]); // 隐含dtype检查
           } else if (stype == kCSRStorage) {
             auto& values = batch.data[data_iter];
             auto& indices = batch.data[data_iter + 1];
             auto& indptr = batch.data[data_iter + 2];
             CHECK_EQ(indices.shape_.Size(), values.shape_.Size());
             // 检查存储空间是否足够
             nd.CheckAndAllocAuxData(csr::kIdx, indices.shape_);
             nd.CheckAndAllocData(values.shape_);
             nd.CheckAndAllocAuxData(csr::kIndPtr, indptr.shape_);
             // copy values, indices and indptr
             CopyFromTo(data_i.data(), values); // 拷贝data，隐含dtype检查
             CopyFromTo(data_i.aux_data(csr::kIdx), indices); // 拷贝特征ID
             CopyFromTo(data_i.aux_data(csr::kIndPtr), indptr); // 拷贝样本偏移位置
           } else {
             LOG(FATAL) << "Storage type not implemented: " << stype;
           }
           data_iter += num_aux_data(stype) + 1;
           (*dptr)->num_batch_padd = batch.num_batch_padd;
         }
         if (batch.inst_index) { // 拷贝样本ID: unsigned => uint64_t
           std::copy(batch.inst_index,
                     batch.inst_index + batch.batch_size,
                     (*dptr)->index.begin());
         }
        return true;
       },
       [this]() { sparse_loader_->BeforeFirst(); });
   }
 
   virtual void BeforeFirst(void) { PrefetcherIter::BeforeFirst(); }
   virtual bool Next(void) { return PrefetcherIter::Next(); }
   virtual const DataBatch &Value(void) const { return PrefetcherIter::Value(); }
 
   virtual const NDArrayStorageType GetStorageType(bool is_data) const {
     return sparse_loader_->GetStorageType(is_data);
   }
   virtual const mxnet::TShape GetShape(bool is_data) const {
     return sparse_loader_->GetShape(is_data);
   }
 
  private:
   /*! \brief internal sparse batch loader */
   SparseIIterator<TBlobBatch>* sparse_loader_;
 
   inline void CopyFromTo(TBlob dst, const TBlob src) {
     MSHADOW_TYPE_SWITCH(src.type_flag_, DType, {
       mshadow::Copy(dst.FlatTo1D<cpu, DType>(), src.FlatTo1D<cpu, DType>());
     });
   }
 };
```
