## ThreadedEnginePooled
* 实现PushToExecute()支持任务按计算task和io task进行区分调度执行，实现线程池执行op；
```c++
class ThreadedEnginePooled : public ThreadedEngine {
 public:
  ThreadedEnginePooled() { this->Start(); } // 启动
  ~ThreadedEnginePooled() noexcept(false) { StopNoWait(); } // 关闭

  void Start() override {
    streams_.reset(new StreamManager<kMaxNumGpus, kNumStreamsPerGpu>());
    /* 构造task队列和io_task队列 */
    task_queue_.reset(new dmlc::ConcurrentBlockingQueue<OprBlock*>());
    io_task_queue_.reset(new dmlc::ConcurrentBlockingQueue<OprBlock*>());
    /* 启动task线程池和io_task线程池 */
    thread_pool_.reset(new ThreadPool(kNumWorkingThreads, // 16个工作线程
                                      [this](std::shared_ptr<dmlc::ManualEvent> ready_event) {
                                        ThreadWorker(task_queue_, ready_event); },
                                      true)); // 等待线程池启动完毕
    io_thread_pool_.reset(new ThreadPool(1, // 一个io线程
                                         [this](std::shared_ptr<dmlc::ManualEvent> ready_event) {
                                           ThreadWorker(io_task_queue_, ready_event); },
                                         true)); // 等待线程池启动完毕
  }

  void StopNoWait() {
    streams_->Finalize();
    task_queue_->SignalForKill();
    io_task_queue_->SignalForKill();
    task_queue_ = nullptr;
    io_task_queue_ = nullptr;
    thread_pool_ = nullptr;
    io_thread_pool_ = nullptr;
    streams_ = nullptr;
  }

  void Stop() override {
    WaitForAll(); // 等待所有pending op调度执行完毕
    StopNoWait(); // 关闭engine
  }

 protected:
  void PushToExecute(OprBlock *opr_block, bool pusher_thread) override {
    if (opr_block->opr->prop == FnProperty::kAsync && pusher_thread) {
      DoExecute(opr_block); // 当前线程立即执行
    } else {
      DoPushToQueue(opr_block); // 按任务类型调度给task队列或io_task队列
    }
  }

 private:
  static constexpr std::size_t kNumWorkingThreads = 16; // 不可配置似乎不大妥当
  static constexpr std::size_t kMaxNumGpus = 16;
  static constexpr std::size_t kNumStreamsPerGpu = 16;

  std::unique_ptr<StreamManager<kMaxNumGpus, kNumStreamsPerGpu>> streams_;
  std::shared_ptr<dmlc::ConcurrentBlockingQueue<OprBlock*>> task_queue_;
  std::shared_ptr<dmlc::ConcurrentBlockingQueue<OprBlock*>> io_task_queue_;
  std::unique_ptr<ThreadPool> thread_pool_;
  std::unique_ptr<ThreadPool> io_thread_pool_;

  void ThreadWorker(std::shared_ptr<dmlc::ConcurrentBlockingQueue<OprBlock*>> task_queue,
                    const std::shared_ptr<dmlc::ManualEvent>& ready_event) {
    OprBlock* opr_block;
    ready_event->signal(); // 通知调用方线程已启动
    while (task_queue->Pop(&opr_block)) { // 从队列获取任务执行
      DoExecute(opr_block);
    }
  }

  void DoPushToQueue(OprBlock* opr_block) {
    switch (opr_block->opr->prop) {
      case FnProperty::kCopyFromGPU:
      case FnProperty::kCopyToGPU: { // io copy task
        io_task_queue_->Push(opr_block);
        break;
      }
      default: { // 计算task
        task_queue_->Push(opr_block);
        break;
      }
    }
  }
  void DoExecute(OprBlock* opr_block) {
#if MXNET_USE_CUDA
    mxnet::common::cuda::DeviceStore device_store(-1, false);
#endif
    assert(opr_block->wait.load() == 0);
    if (opr_block->ctx.dev_mask() == gpu::kDevMask) {
      #if MXNET_USE_CUDA
      device_store.SetDevice(opr_block->ctx.dev_id);
      #else   // MXNET_USE_CUDA
      LOG(FATAL) << "Please compile with CUDA enabled";
      #endif  // MXNET_USE_CUDA
    }
    bool is_copy = (opr_block->opr->prop == FnProperty::kCopyFromGPU ||
                    opr_block->opr->prop == FnProperty::kCopyToGPU);
    auto&& rctx = is_copy
        ? streams_->GetIORunContext(opr_block->ctx)
        : streams_->GetRunContext(opr_block->ctx);
    this->ExecuteOprBlock(rctx, opr_block); // 转调ThreadedEngine::ExecuteOprBlock()
  }
};
```
